# üìÑ What Does It Do?

This is a simple text classification model based on the **Multinomial Distribution** and **Bayes' Theorem**. It estimates the probability that a given article was written by **Author A** or **Author B**.

---

# ‚öôÔ∏è How It Works

### üßæ Inputs

The model takes **three inputs**:

- `text_A.txt`: Sample text(s) known to be written by Author A  
- `text_B.txt`: Sample text(s) known to be written by Author B  
- `text.txt`: The unknown article to classify

You will also be asked to input your **prior belief** \( P(A) \) ‚Äî i.e., how likely you think the article is written by A (enter `0.5` if you're unsure).

---

### üé≤ Random Variables

- **A**: The author is A  
- **B**: The author is B  
- **T**: The text is the one we want to classify

---

### üìê Core Equation

We compute the posterior odds ratio:

$$
\frac{P(A \mid T)}{P(B \mid T)} = \frac{P(T \mid A)}{P(T \mid B)} \cdot \frac{P(A)}{P(B)}
$$

Assuming a multinomial distribution for the text, we approximate:

$$
\frac{P(T \mid A)}{P(T \mid B)} =
\frac{{p_1}^{c_1} \cdot {p_2}^{c_2} \cdots {p_m}^{c_m}}{{q_1}^{c_1} \cdot {q_2}^{c_2} \cdots {q_m}^{c_m}}
$$

Where:  
$p_i$: probability of word $i$ in Author A‚Äôs texts  
$q_i$: probability of word $i$ in Author B‚Äôs texts  
$c_i$: count of word $i$ in the unknown text

---

### **üõ°Ô∏è Handling Small Probabilities (Underflow)**



When $p_i$ and $q_i$ are very small, multiplying many of them can lead to **floating-point underflow** ‚Äî where the result is too tiny for the computer to represent accurately.



To avoid this, we take the **logarithm of the likelihood ratio**:
  

$$
\log \left( \frac{P(T \mid A)}{P(T \mid B)} \right) = \sum_{i} c_i \cdot \log \left( \frac{p_i}{q_i} \right)
$$

  

Then, after summing in log space, we exponentiate to recover the original ratio:

  
$$
\frac{P(T \mid A)}{P(T \mid B)} = \exp\left( \sum_i c_i \cdot \log \left( \frac{p_i}{q_i} \right) \right)
$$
  

This is **numerically stable** and avoids underflow issues.

---

### **üõ°Ô∏è Handling Extremely Large Ratios**

  

If the likelihood $ratio \frac{P(T \mid A)}{P(T \mid B)}$ becomes **extremely large or small**, it can lead to **numerical overflow** or cause instability in the final probability calculation.

  

To ensure stability, we **clip** the log-likelihood ratio to a reasonable range, such as:

  
$$
logratio = \min(\max(logratio, -100), 100)
$$
  

This keeps the final posterior probability well-defined:

  
$$
P(A \mid T) = \frac{\exp(logratio) \cdot P(A)}{\exp(logratio) \cdot P(A) + P(B)}
$$
  

This prevents the model from outputting probabilities of exactly 0 or 1 due to extreme ratios, and ensures **smoother, more interpretable results**.
